{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Schedulers\n",
    "\n",
    "In this notebook we demonstrate how to work with dask dataframes. \n",
    "\n",
    "* A few words about dask schedulars\n",
    "* Dask Schedulars on a single machine\n",
    "    * local thread\n",
    "    * local processes\n",
    "    * single thread\n",
    "* Apply schedular options to weather station data\n",
    "* Distributed schedulars (local)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Authors: NCI Virtual Research Environment Team\n",
    "- Keywords: Dask, Dataframe, schedulars\n",
    "- Create Date: 2020-May\n",
    "- Lineage/Reference: This tutorial is referenced to [dask tutorial](https://github.com/dask/dask-tutorial).\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "\n",
    "The following modules are needed:\n",
    "\n",
    "* Pandas\n",
    "* Dask\n",
    "* Pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedulers\n",
    "\n",
    "In the previous notebooks, we used `dask.delayed` and `dask.dataframe` to parallelize computations.\n",
    "These work by building a *task graph* instead of executing immediately.\n",
    "Each *task* represents some function to call on some data, and the full *graph* is the relationship between all the tasks.\n",
    "\n",
    "When we wanted the actual result, we called `compute`, which handed the task graph off to a *scheduler*.\n",
    "\n",
    "**Schedulers are responsible for running a task graph and producing a result**.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dask/dask-org/master/images/grid_search_schedule.gif)\n",
    "\n",
    "First, there are the single machine schedulers that execute things in parallel using threads or processes (or synchronously for debugging). These are what we've used up until now. Second, there's the `dask.distributed` scheduler, which is newer and has more features than the single machine scheduler.\n",
    "\n",
    "In this notebook we'll first talk about the different schedulers. Then we'll use the `dask.distributed` scheduler in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Schedulers\n",
    "\n",
    "Dask separates computation description (task graphs) from execution (schedulers). This allows you to write code once, and run it locally or scale it out across a cluster.\n",
    "\n",
    "Dask has two families of task schedulers:\n",
    "\n",
    "- Single machine scheduler: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale.\n",
    "\n",
    "- Distributed scheduler: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster.\n",
    "\n",
    "For different computations you may find better performance with particular scheduler settings. This session helps you understand how to choose between and configure different schedulers, and provides guidelines on when one might be more appropriate.\n",
    "\n",
    "#### Local Threads\n",
    "\n",
    "```python\n",
    "- `dask.config.set(scheduler='threads')`  # overwrite default with threaded scheduler\n",
    "```\n",
    "\n",
    "The threaded scheduler executes computations with a local multiprocessing.pool.ThreadPool. It is lightweight and requires no setup. It introduces very little task overhead (around 50us per task) and, because everything occurs in the same process, it incurs no costs to transfer data between tasks. However, due to Pythonâ€™s Global Interpreter Lock (GIL), this scheduler only provides parallelism when your computation is dominated by non-Python code, as is primarily the case when operating on numeric data in NumPy arrays, Pandas DataFrames, or using any of the other C/C++/Cython based projects in the ecosystem.\n",
    "\n",
    "The threaded scheduler is the default choice for Dask Array, Dask DataFrame, and Dask Delayed. However, if your computation is dominated by processing pure Python objects like strings, dicts, or lists, then you may want to try one of the process-based schedulers below (we currently recommend the distributed scheduler on a local machine).\n",
    "\n",
    "#### Local Processes\n",
    "\n",
    "```python\n",
    "import dask.multiprocessing\n",
    "dask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n",
    "```\n",
    "\n",
    "The multiprocessing scheduler executes computations with a local multiprocessing.Pool. It is lightweight to use and requires no setup. Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process. This means that it is able to bypass issues with the GIL and provide parallelism even on computations that are dominated by pure Python code, such as those that process strings, dicts, and lists.\n",
    "\n",
    "However, moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. The multiprocessing scheduler is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts.\n",
    "\n",
    "#### Single Thread\n",
    "\n",
    "```python\n",
    "import dask\n",
    "dask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n",
    "```\n",
    "\n",
    "The single-threaded synchronous scheduler executes all computations in the local thread with no parallelism at all. This is particularly valuable for debugging and profiling, which are more difficult when using threads or processes.\n",
    "\n",
    "For example, when using IPython or Jupyter notebooks, the `%debug`, `%pdb`, or `%prun` magics will not work well when using the parallel Dask schedulers (they were not designed to be used in a parallel computing context). However, if you run into an exception and want to step into the debugger, you may wish to rerun your computation under the single-threaded scheduler where these tools will function properly.\n",
    "\n",
    "Here we discuss the *local* schedulers - schedulers that run only on a single machine. In each case we change the scheduler used in a few different ways:\n",
    "\n",
    "- By providing a `scheduler=` keyword argument to `compute`:\n",
    "\n",
    "```python\n",
    "max_rain.compute(scheduler='processes'))\n",
    "# or \n",
    "max_rain.compute(scheduler='synchronous')\n",
    "```\n",
    "\n",
    "- Using `dask.set_options`:\n",
    "\n",
    "```python\n",
    "# Use multiprocessing in this block\n",
    "with dask.set_options(scheduler='processes'):\n",
    "    max_rain.compute()\n",
    "# Use multiprocessing globally\n",
    "dask.set_options(scheduler='synchronous')\n",
    "```\n",
    "\n",
    "Here we repeat a simple dataframe computation from the previous section using the different schedulers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'weather_stations_ACT','IDCJAC0009_*_*','IDCJAC0009*.csv')\n",
    "df = dd.read_csv(filename)\n",
    "# rename column headers\n",
    "df.columns = ['code','station','year','month','day','rainfall','period','quality']\n",
    "# Maximum rainfall\n",
    "max_rain=df.rainfall.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=float64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 s, sys: 853 ms, total: 4.81 s\n",
      "Wall time: 2.63 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = max_rain.compute()  # this uses threads by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 427 ms, sys: 304 ms, total: 731 ms\n",
      "Wall time: 800 ms\n"
     ]
    }
   ],
   "source": [
    "import dask.multiprocessing\n",
    "%time _ = max_rain.compute(scheduler='processes')  # this uses processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 177 ms, total: 1.4 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = max_rain.compute(scheduler='synchronous')  # This uses a single thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the threaded and multiprocessing schedulers use the same number of workers as cores. You can change this using the `num_workers` keyword in the same way that you specified `scheculer` above:\n",
    "\n",
    "```\n",
    "max_rain.compute(scheduler='processes', num_workers=2)\n",
    "```\n",
    "\n",
    "To see how many cores you have on your computer, you can use `multiprocessing.cpu_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Questions to Consider:\n",
    "\n",
    "- How much speedup is possible for this task (hint, look at the graph).\n",
    "- Given how many cores are on this machine, how much faster could the parallel schedulers be than the single-threaded scheduler.\n",
    "- How much faster was using threads over a single thread? Why does this differ from the optimal speedup?\n",
    "- Why is the multiprocessing scheduler so much slower here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In what cases would you want to use one scheduler over another?\n",
    "\n",
    "http://dask.pydata.org/en/latest/setup/single-machine.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Scheduler\n",
    "\n",
    "The `dask.distributed` system is composed of a single centralized scheduler and many worker processes. [Deploying](http://dask.pydata.org/en/latest/setup.html) a remote Dask cluster involves some additional effort. But doing things locally is just involves creating a `Client` object, which lets you interact with the \"cluster\" (local threads or processes on your machine). For more information see [here](http://dask.pydata.org/en/latest/setup/single-distributed.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>LocalCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "LocalCluster('tcp://127.0.0.1:36763', workers=8, threads=48, memory=202.49 GB)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Setup a local cluster.\n",
    "# By default this sets up 1 worker per core\n",
    "client = Client()\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to click the `Dashboard` link to open up the diagnostics dashboard. If you run this notebook under Pangeo environment on Gadi, the server port number is in the second line of the `client.cmd` file.\n",
    "\n",
    "By default, creating a `Client` makes it the default scheduler. Any calls to `.compute` will use the cluster your `client` is attached to (See http://dask.pydata.org/en/latest/scheduling.html for how to specify which scheduler to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 ms, sys: 55.4 ms, total: 829 ms\n",
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "322.1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time max_rain.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Questions to Consider\n",
    "\n",
    "- How does this compare to the optimal parallel speedup?\n",
    "- Why is this faster than the threaded scheduler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the following computations while looking at the diagnostics page. In each case what is taking the most time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.) How many rows are in our dataset?\n",
    "_ = len(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.) In total, how many days records were taken?\n",
    "_ = pd.read_parquet('data/ACT_weather.parquet', columns=['period'], engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.) In total, how many non-record days from each weather station?\n",
    "_ = dd.groupby(\"station\").period.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.) What was the average rainfall from each station?\n",
    "_ = dd.groupby(\"station\").rainfall.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More...\n",
    "\n",
    "The distributed scheduler is more sophisticated than the single machine schedulers. It can compute asynchronously, and also provides an api similar to that of `concurrent.futures`. For further information you can see the docs http://distributed.readthedocs.io/en/latest/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://docs.dask.org/en/latest/scheduling.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
